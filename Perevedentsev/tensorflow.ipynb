{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google TensorFlow\n",
    "<img src=\"logo.png\" alt=\"logo\" style=\"width: 40%;\"/>\n",
    "\n",
    "## Overview\n",
    "\n",
    "TensorFlow is a programming system in which you represent computations as graphs. Nodes in the graph are called ops (short for operations). An op takes zero or more Tensors, performs some computation, and produces zero or more Tensors. A Tensor is a typed multi-dimensional array. For example, you can represent a mini-batch of images as a 4-D array of floating point numbers with dimensions [batch, height, width, channels].\n",
    "\n",
    "A TensorFlow graph is a description of computations. To compute anything, a graph must be launched in a Session. A Session places the graph ops onto Devices, such as CPUs or GPUs, and provides methods to execute them. These methods return tensors produced by ops as numpy ndarray objects in Python, and as tensorflow::Tensor instances in C and C++.\n",
    "\n",
    "<img src=\"chart.png\" alt=\"logo\" style=\"width: 40%;\"/>\n",
    "\n",
    "## Installation\n",
    "\n",
    "- Yet no support for GPU on OS X\n",
    "- Can be hardly installed on Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "# Ubuntu/Linux 64-bit, CPU only:\n",
    "# pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n",
    "# Ubuntu/Linux 64-bit, GPU enabled:\n",
    "# pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n",
    "# Mac OS X, CPU only:\n",
    "easy_install --upgrade six\n",
    "pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the default graph\n",
    "\n",
    "To run the matmul op we call the session 'run()' method, passing 'product'\n",
    "which represents the output of the matmul op.  This indicates to the call\n",
    "that we want to get the output of the matmul op back.\n",
    "\n",
    "All inputs needed by the op are run automatically by the session.  They\n",
    "typically are run in parallel.\n",
    "\n",
    "The call 'run(product)' thus causes the execution of threes ops in the\n",
    "graph: the two constants and matmul.\n",
    "\n",
    "The output of the op is returned in 'result' as a numpy `ndarray` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 12.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "    # added as a node to the default graph.\n",
    "    #\n",
    "    # The value returned by the constructor represents the output\n",
    "    # of the Constant op.\n",
    "    matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "    # Create another Constant that produces a 2x1 matrix.\n",
    "    matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "    # Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "    # The returned value, 'product', represents the result of the matrix\n",
    "    # multiplication.\n",
    "    product = tf.matmul(matrix1, matrix2)\n",
    "    with tf.Session() as sess:\n",
    "        result = sess.run([product])\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an opportunity to explicitly select computational device for session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 12.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            matrix1 = tf.constant([[3., 3.]])\n",
    "            matrix2 = tf.constant([[2.],[2.]])\n",
    "            product = tf.matmul(matrix1, matrix2)\n",
    "            result = sess.run([product])\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do it in a parallel way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    c = []\n",
    "    for d in ['/gpu:2', '/gpu:3']:\n",
    "        with tf.device(d):\n",
    "            a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "            b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "            c.append(tf.matmul(a, b))\n",
    "    with tf.device('/cpu:0'):\n",
    "        sum = tf.add_n(c)\n",
    "    # Creates a session with log_device_placement set to True.\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    # Runs the op.\n",
    "    print(sess.run(sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "Variables maintain state across executions of the graph. The following example shows a variable serving as a simple counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # Create a Variable, that will be initialized to the scalar value 0.\n",
    "    state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "    # Create an Op to add one to `state`.\n",
    "\n",
    "    one = tf.constant(1)\n",
    "    new_value = tf.add(state, one)\n",
    "    update = tf.assign(state, new_value)\n",
    "\n",
    "    # Variables must be initialized by running an `init` Op after having\n",
    "    # launched the graph.  We first have to add the `init` Op to the graph.\n",
    "    init_op = tf.initialize_all_variables()\n",
    "\n",
    "    # Launch the graph and run the ops.\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        print(sess.run(state))\n",
    "        for _ in range(3):\n",
    "            sess.run(update)\n",
    "            print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"chart2.png\" alt=\"logo\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assign() operation in this code is a part of the expression graph just like the add() operation, so it does not actually perform the assignment until run() executes the expression.\n",
    "\n",
    "You typically represent the parameters of a statistical model as a set of Variables. For example, you would store the weights for a neural network as a tensor in a Variable. During training you update this tensor by running a training graph repeatedly.\n",
    "\n",
    "### Explore linear equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.26455051] [ 0.90916252]\n",
      "20 [-0.00258814] [ 0.35509714]\n",
      "40 [ 0.08039634] [ 0.3105284]\n",
      "60 [ 0.09625393] [ 0.30201188]\n",
      "80 [ 0.09928416] [ 0.30038446]\n",
      "100 [ 0.0998632] [ 0.30007347]\n",
      "120 [ 0.09997385] [ 0.30001405]\n",
      "140 [ 0.09999499] [ 0.30000269]\n",
      "160 [ 0.09999905] [ 0.30000052]\n",
      "180 [ 0.09999983] [ 0.3000001]\n",
      "200 [ 0.0999999] [ 0.30000007]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(10000).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "\n",
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but Tensorflow will\n",
    "# figure that out for us.)\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b\n",
    "\n",
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.6)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph.\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Fit the line.\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "There is an opportunity to control and visualize your computational graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.train.SummaryWriter('./tfdata/', sess.graph_def);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tensorboard --logdir ./tfdata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST example\n",
    "\n",
    "<img src=\"chart3.png\" alt=\"logo\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow.python.platform\n",
    "import tensorflow as tf\n",
    "NUM_CLASSES = 10\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "def inference(images, hidden1_units, hidden2_units):\n",
    "  # Hidden 1\n",
    "  with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                         name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "  # Hidden 2\n",
    "  with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                         name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  # Linear\n",
    "  with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                         name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "  return logits\n",
    "\n",
    "def loss_function(logits, labels):\n",
    "  batch_size = tf.size(labels)\n",
    "  labels = tf.expand_dims(labels, 1)\n",
    "  indices = tf.expand_dims(tf.range(0, batch_size), 1)\n",
    "  concated = tf.concat(1, [indices, labels])\n",
    "  onehot_labels = tf.sparse_to_dense(\n",
    "      concated, tf.pack([batch_size, NUM_CLASSES]), 1.0, 0.0)\n",
    "  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,\n",
    "                                                          onehot_labels,\n",
    "                                                          name='xentropy')\n",
    "  loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "  return loss\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "  tf.scalar_summary(loss.op.name, loss)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  return train_op\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "  correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting tfdata/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting tfdata/mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting tfdata/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting tfdata/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: loss = 2.31 (0.054 sec)\n",
      "Step 100: loss = 1.96 (0.023 sec)\n",
      "Step 200: loss = 1.56 (0.022 sec)\n",
      "Step 300: loss = 1.10 (0.023 sec)\n",
      "Step 400: loss = 0.81 (0.023 sec)\n",
      "Step 500: loss = 0.74 (0.023 sec)\n",
      "Step 600: loss = 0.66 (0.022 sec)\n",
      "Step 700: loss = 0.57 (0.028 sec)\n",
      "Step 800: loss = 0.53 (0.024 sec)\n",
      "Step 900: loss = 0.43 (0.030 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 48744  Precision @ 1: 0.8863\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4465  Precision @ 1: 0.8930\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 8931  Precision @ 1: 0.8931\n",
      "Step 1000: loss = 0.37 (0.039 sec)\n",
      "Step 1100: loss = 0.33 (0.133 sec)\n",
      "Step 1200: loss = 0.34 (0.038 sec)\n",
      "Step 1300: loss = 0.35 (0.027 sec)\n",
      "Step 1400: loss = 0.38 (0.026 sec)\n",
      "Step 1500: loss = 0.38 (0.026 sec)\n",
      "Step 1600: loss = 0.32 (0.029 sec)\n",
      "Step 1700: loss = 0.25 (0.025 sec)\n",
      "Step 1800: loss = 0.39 (0.026 sec)\n",
      "Step 1900: loss = 0.36 (0.043 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49940  Precision @ 1: 0.9080\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4577  Precision @ 1: 0.9154\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9145  Precision @ 1: 0.9145\n",
      "Step 2000: loss = 0.38 (0.045 sec)\n",
      "Step 2100: loss = 0.29 (0.026 sec)\n",
      "Step 2200: loss = 0.38 (0.132 sec)\n",
      "Step 2300: loss = 0.41 (0.028 sec)\n",
      "Step 2400: loss = 0.32 (0.026 sec)\n",
      "Step 2500: loss = 0.36 (0.026 sec)\n",
      "Step 2600: loss = 0.27 (0.026 sec)\n",
      "Step 2700: loss = 0.27 (0.026 sec)\n",
      "Step 2800: loss = 0.33 (0.026 sec)\n",
      "Step 2900: loss = 0.25 (0.026 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 50588  Precision @ 1: 0.9198\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4625  Precision @ 1: 0.9250\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9233  Precision @ 1: 0.9233\n",
      "Step 3000: loss = 0.32 (0.042 sec)\n",
      "Step 3100: loss = 0.35 (0.027 sec)\n",
      "Step 3200: loss = 0.27 (0.027 sec)\n",
      "Step 3300: loss = 0.31 (0.142 sec)\n",
      "Step 3400: loss = 0.25 (0.030 sec)\n",
      "Step 3500: loss = 0.29 (0.027 sec)\n",
      "Step 3600: loss = 0.30 (0.024 sec)\n",
      "Step 3700: loss = 0.24 (0.026 sec)\n",
      "Step 3800: loss = 0.22 (0.029 sec)\n",
      "Step 3900: loss = 0.20 (0.027 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 50990  Precision @ 1: 0.9271\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4660  Precision @ 1: 0.9320\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9310  Precision @ 1: 0.9310\n",
      "Step 4000: loss = 0.28 (0.041 sec)\n",
      "Step 4100: loss = 0.29 (0.026 sec)\n",
      "Step 4200: loss = 0.24 (0.032 sec)\n",
      "Step 4300: loss = 0.37 (0.026 sec)\n",
      "Step 4400: loss = 0.25 (0.135 sec)\n",
      "Step 4500: loss = 0.31 (0.026 sec)\n",
      "Step 4600: loss = 0.25 (0.025 sec)\n",
      "Step 4700: loss = 0.28 (0.025 sec)\n",
      "Step 4800: loss = 0.24 (0.025 sec)\n",
      "Step 4900: loss = 0.21 (0.026 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 51372  Precision @ 1: 0.9340\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4695  Precision @ 1: 0.9390\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9376  Precision @ 1: 0.9376\n",
      "Step 5000: loss = 0.26 (0.043 sec)\n",
      "Step 5100: loss = 0.19 (0.026 sec)\n",
      "Step 5200: loss = 0.21 (0.025 sec)\n",
      "Step 5300: loss = 0.24 (0.028 sec)\n",
      "Step 5400: loss = 0.34 (0.028 sec)\n",
      "Step 5500: loss = 0.22 (0.136 sec)\n",
      "Step 5600: loss = 0.22 (0.027 sec)\n",
      "Step 5700: loss = 0.20 (0.025 sec)\n",
      "Step 5800: loss = 0.31 (0.025 sec)\n",
      "Step 5900: loss = 0.29 (0.025 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 51730  Precision @ 1: 0.9405\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4724  Precision @ 1: 0.9448\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9417  Precision @ 1: 0.9417\n",
      "Step 6000: loss = 0.22 (0.043 sec)\n",
      "Step 6100: loss = 0.29 (0.025 sec)\n",
      "Step 6200: loss = 0.21 (0.026 sec)\n",
      "Step 6300: loss = 0.17 (0.027 sec)\n",
      "Step 6400: loss = 0.18 (0.027 sec)\n",
      "Step 6500: loss = 0.16 (0.028 sec)\n",
      "Step 6600: loss = 0.19 (0.142 sec)\n",
      "Step 6700: loss = 0.24 (0.031 sec)\n",
      "Step 6800: loss = 0.18 (0.025 sec)\n",
      "Step 6900: loss = 0.18 (0.025 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 51962  Precision @ 1: 0.9448\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4747  Precision @ 1: 0.9494\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9454  Precision @ 1: 0.9454\n",
      "Step 7000: loss = 0.20 (0.040 sec)\n",
      "Step 7100: loss = 0.23 (0.026 sec)\n",
      "Step 7200: loss = 0.30 (0.026 sec)\n",
      "Step 7300: loss = 0.21 (0.026 sec)\n",
      "Step 7400: loss = 0.21 (0.027 sec)\n",
      "Step 7500: loss = 0.15 (0.027 sec)\n",
      "Step 7600: loss = 0.11 (0.027 sec)\n",
      "Step 7700: loss = 0.21 (0.137 sec)\n",
      "Step 7800: loss = 0.14 (0.025 sec)\n",
      "Step 7900: loss = 0.14 (0.026 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 52170  Precision @ 1: 0.9485\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4760  Precision @ 1: 0.9520\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9483  Precision @ 1: 0.9483\n",
      "Step 8000: loss = 0.20 (0.041 sec)\n",
      "Step 8100: loss = 0.15 (0.026 sec)\n",
      "Step 8200: loss = 0.23 (0.026 sec)\n",
      "Step 8300: loss = 0.16 (0.025 sec)\n",
      "Step 8400: loss = 0.14 (0.026 sec)\n",
      "Step 8500: loss = 0.14 (0.026 sec)\n",
      "Step 8600: loss = 0.17 (0.026 sec)\n",
      "Step 8700: loss = 0.25 (0.026 sec)\n",
      "Step 8800: loss = 0.16 (0.135 sec)\n",
      "Step 8900: loss = 0.24 (0.030 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 52480  Precision @ 1: 0.9542\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4774  Precision @ 1: 0.9548\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9507  Precision @ 1: 0.9507\n",
      "Step 9000: loss = 0.18 (0.042 sec)\n",
      "Step 9100: loss = 0.21 (0.027 sec)\n",
      "Step 9200: loss = 0.17 (0.027 sec)\n",
      "Step 9300: loss = 0.20 (0.027 sec)\n",
      "Step 9400: loss = 0.19 (0.026 sec)\n",
      "Step 9500: loss = 0.13 (0.026 sec)\n",
      "Step 9600: loss = 0.14 (0.026 sec)\n",
      "Step 9700: loss = 0.17 (0.028 sec)\n",
      "Step 9800: loss = 0.18 (0.028 sec)\n",
      "Step 9900: loss = 0.11 (0.137 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 52590  Precision @ 1: 0.9562\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4789  Precision @ 1: 0.9578\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9532  Precision @ 1: 0.9532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object get_controller at 0x1168186d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2791, in get_controller\n",
      "    assert self.stack[-1] is default\n",
      "AssertionError: \n",
      "Exception ignored in: <generator object get_controller at 0x108cf1e60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2791, in get_controller\n",
      "    assert self.stack[-1] is default\n",
      "AssertionError: \n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import time\n",
    "import tensorflow.python.platform\n",
    "import numpy\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "learning_rate = 0.01\n",
    "max_steps = 10000\n",
    "hidden1 = 1024\n",
    "hidden2 = 512\n",
    "batch_size = 200\n",
    "train_dir = 'tfdata/mnist'\n",
    "fake_data = False\n",
    "\n",
    "def placeholder_inputs(batch_size):\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                         IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  return images_placeholder, labels_placeholder\n",
    "\n",
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "  images_feed, labels_feed = data_set.next_batch(batch_size,\n",
    "                                                 fake_data)\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict\n",
    "\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "  true_count = 0\n",
    "  steps_per_epoch = data_set.num_examples // batch_size\n",
    "  num_examples = steps_per_epoch * batch_size\n",
    "  for step in xrange(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = true_count / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n",
    "    \n",
    "def run_training():\n",
    "  data_sets = input_data.read_data_sets(train_dir, fake_data)\n",
    "  with tf.Graph().as_default():\n",
    "    # Generate placeholders for the images and labels.\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(batch_size)\n",
    "    logits = inference(images_placeholder,\n",
    "                             hidden1,\n",
    "                             hidden2)\n",
    "    loss = loss_function(logits, labels_placeholder)\n",
    "    train_op = training(loss, learning_rate)\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.train.SummaryWriter(train_dir,\n",
    "                                            graph_def=sess.graph_def)\n",
    "    # And then after everything is built, start the training loop.\n",
    "    for step in xrange(max_steps):\n",
    "      start_time = time.time()\n",
    "      feed_dict = fill_feed_dict(data_sets.train,\n",
    "                                 images_placeholder,\n",
    "                                 labels_placeholder)\n",
    "      _, loss_value = sess.run([train_op, loss],\n",
    "                               feed_dict=feed_dict)\n",
    "      duration = time.time() - start_time\n",
    "      if step % 100 == 0:\n",
    "        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "        summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "      if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "        saver.save(sess, train_dir, global_step=step)\n",
    "        print('Training Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.train)\n",
    "        print('Validation Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.validation)\n",
    "        print('Test Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                data_sets.test)\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
