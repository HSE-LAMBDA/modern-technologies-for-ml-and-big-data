{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.mllib.classification._\n",
    "import org.apache.spark.mllib.regression._\n",
    "import org.apache.spark.mllib.linalg._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadCsv(path: String): RDD[LabeledPoint] = {\n",
    "    import scala.util.Try\n",
    "\n",
    "    sc.textFile(path).flatMap { line =>\n",
    "        Try {\n",
    "            val row = line.split(\",\").map { _.toInt }\n",
    "            LabeledPoint(label = row.head, features = new DenseVector(row.tail.map { _.toDouble }))\n",
    "        }.toOption\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at randomSplit at <console>:31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = loadCsv(\"mnist_train.csv\").randomSplit(Array(0.8, 0.2), seed=333L)\n",
    "\n",
    "/** We will use training and test datsets more than once.\n",
    " *   To avoid recalculation they should be persisted (cached).\n",
    " */\n",
    "train.persist()\n",
    "test.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/** Default logistic regression. **/\n",
    "val logreg = new LogisticRegressionWithLBFGS().setNumClasses(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/** The same could be done by `LogisticRegression.train` methods **/\n",
    "val trainedModel = logreg.run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/** Construction `arg: { def f(...): ... }` means any class that contains\n",
    " *   a method with the same name and signature.\n",
    " */\n",
    "def testAccuracy(model: { def predict(f: Vector): Double })(data: RDD[LabeledPoint]): Double = {\n",
    "    val prediction = data.map { lp =>\n",
    "        val pred = model.predict(lp.features)\n",
    "        (lp.label, pred)\n",
    "    }\n",
    "    \n",
    "    import org.apache.spark.mllib.evaluation._\n",
    "    val metrics = new MulticlassMetrics(prediction)\n",
    "    val cm = metrics.confusionMatrix\n",
    "    \n",
    "    val correct = (0 until cm.numCols).map { i => cm(i, i) }.sum\n",
    "    val total = cm.toArray.sum\n",
    "    \n",
    "    correct / total\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println {\n",
    "    testAccuracy(trainedModel)(test)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val logreg = new LogisticRegressionWithLBFGS().setNumClasses(10)\n",
    "logreg.optimizer.setRegParam(0.1)\n",
    "\n",
    "val trainedModel = logreg.run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println {\n",
    "    testAccuracy(trainedModel)(test)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rocauc(model: { def predict(f: Vector): Double })(data: RDD[LabeledPoint]): Double = {\n",
    "    val prediction = data.map { lp =>\n",
    "        val pred = model.predict(lp.features)\n",
    "        (lp.label, pred)\n",
    "    }\n",
    "    \n",
    "    import org.apache.spark.mllib.evaluation._\n",
    "    val metrics = new BinaryClassificationMetrics(prediction)\n",
    "    metrics.areaUnderROC()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val digit0_train = train.map { lp => lp.copy(label = if (lp.label == 0) 1.0 else 0.0) }\n",
    "val digit0_test = test.map { lp => lp.copy(label = if (lp.label == 0) 1.0 else 0.0) }\n",
    "\n",
    "digit0_train.persist()\n",
    "digit0_test.persist()\n",
    "\n",
    "val svm = SVMWithSGD.train(digit0_train, numIterations=250, stepSize=0.1, regParam=0.1, miniBatchFraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9478939639502503"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc(svm)(digit0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833177788147457"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAccuracy(svm)(digit0_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.tree.RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val rf = RandomForest.trainClassifier(input = train, numClasses=10,\n",
    "    categoricalFeaturesInfo=Map.empty[Int, Int],\n",
    "    numTrees=5,\n",
    "    featureSubsetStrategy=\"log2\", impurity =\"gini\", maxDepth=25, maxBins = 100, seed=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.886724218385441"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAccuracy(rf)(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.tree._\n",
    "import org.apache.spark.mllib.tree.configuration._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val boostingStrategy = BoostingStrategy.defaultParams(\"Classification\")\n",
    "\n",
    "boostingStrategy.numIterations = 2\n",
    "boostingStrategy.treeStrategy.numClasses = 2\n",
    "boostingStrategy.treeStrategy.maxDepth = 25\n",
    "\n",
    "boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map.empty[Int, Int]\n",
    "\n",
    "println {\n",
    "    boostingStrategy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val gbt = GradientBoostedTrees.train(train, boostingStrategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9540873460246361"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAccuracy(gbt)(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBT regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val boostingStrategy = BoostingStrategy.defaultParams(\"Regression\")\n",
    "\n",
    "boostingStrategy.numIterations = 2\n",
    "boostingStrategy.treeStrategy.maxDepth = 25\n",
    "\n",
    "boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map.empty[Int, Int]\n",
    "\n",
    "println {\n",
    "    boostingStrategy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val gbt = GradientBoostedTrees.train(train, boostingStrategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15212319178721417"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAccuracy(gbt)(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10.4",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
